---
title: "Bicep curl quality"
author: "Chris Lill"
date: "17 September 2015"
output: html_document
---

# Executive summary
A random forest model has been created to predict the quality of a bicep curl based on sensor data. This model was tuned and is expected to have an out of sample error of ?????

The original study that generated this data is [Wearable Computing: Accelerometers' Data Classification of Body Postures and Movements](http://groupware.les.inf.puc-rio.br/work.jsf?p1=10335) by Ugulino, Cardador, Vega, Velloso, Milidiu, and Fuks. The study categorised bicep curls in the following categories:

* A - exactly according to the specification 
* B - throwing the elbows to the front
* C - lifting the dumbbell only halfway
* D - lowering the dumbbell only halfway
* E - throwing the hips to the front

# Data preparation
The project supplied a large training set, and a small set of 20 records to be predict, which will be used as follows:

* `bc.train` contains 70% of the large dataset to train the model,
* `bc.test` contains 30% of the large dataset to tune the model,
* `bc.validation` contains the 20 records to be predicted.

Initial investigation shows that the study derived many factors from the original data where 98% of the data is NA or blank. These factors can be removed from the model, along with all metadata, leaving 52 factors for modelling.

```{r Download, cache = TRUE, echo = FALSE}
train.file <- "pml-training.csv"
validation.file <- "pml-testing.csv"
if (!file.exists(train.file)) {
folder.url <- "https://d396qusza40orc.cloudfront.net/predmachlearn/"
    download.file(paste(folder.url,train.file, sep = ""), train.file)
    download.file(paste(folder.url,validation.file, sep = ""), validation.file)
    date.downloaded = Sys.Date()
}

full.train <- read.csv(train.file)
bc.validation <- read.csv(validation.file)
```

```{r RemoveNullFactors, cache = TRUE, echo = FALSE}
all.factors <- names(full.train)
remove.names <- c("X",
                  "user_name",
                  "num_window",
                  "raw_timestamp_part_1",
                  "raw_timestamp_part_2",
                  "cvtd_timestamp",
                  "new_window",
                  "*_roll_belt",
                  "*_yaw_belt",
                  "*_pitch_belt",
                  "*_picth_belt",
                  "*_roll_arm",
                  "*_pitch_arm",
                  "*_picth_arm",
                  "*_yaw_arm",
                  "*_roll_dumbbell",
                  "*_pitch_dumbbell",
                  "*_picth_dumbbell",
                  "*_yaw_dumbbell",
                  "*_roll_forearm",
                  "*_pitch_forearm",
                  "*_picth_forearm",
                  "*_yaw_forearm",
                  "var_accel_",
                  "var_total_accel_belt")
remove.regex <- paste(remove.names, collapse = "|")
remove.factors <- grepl(remove.regex, all.factors)

full.train <- full.train[,!remove.factors]
```

```{r SplitData, cache = TRUE, echo = FALSE}
library(caret, quietly = TRUE)
set.seed(29)
inTrain <- createDataPartition(full.train$classe, p = 0.7, list = FALSE)
bc.train <- full.train[inTrain, -53]
bc.test <- full.train[-inTrain, -53]
classe.train <- full.train[inTrain, 53]
classe.test <- full.train[-inTrain, 53]
```

# Exploratory data analysis

The following plots show the variability by classification for six factors with the greatest importance in the final model. They show that there are visible differences, but that a combination of factors will be needed for an accurate prediction.

```{r Exploratory, cache = TRUE, echo = FALSE, fig.width=10}
library(ggplot2, quietly = TRUE)
library(gridExtra)

b1 <- geom_boxplot(aes(colour = classe))
b2 <- theme(legend.position="none", axis.title.x = element_blank())
b3 <- scale_colour_brewer(palette = "Set1")

g1 <- ggplot(full.train, aes(classe, roll_belt)) + b1 + b2 + b3
g2 <- ggplot(full.train, aes(classe, yaw_belt)) + b1 + b2 + b3
g3 <- ggplot(full.train, aes(classe, pitch_belt)) + b1 + b2 + b3
g4 <- ggplot(full.train, aes(classe, magnet_dumbbell_z)) + b1 + b2 + b3
g5 <- ggplot(full.train, aes(classe, magnet_dumbbell_x)) + b1 + b2 + b3
g6 <- ggplot(full.train, aes(classe, pitch_forearm)) + b1 + b2 + b3
grid.arrange(g1, g2, g3, g4, g5, g6, ncol = 3, bottom = "Figure 1: Boxplots of six important factors by classification")
```

# Model selection

8. Quirk of data that enables 99% accuracy
"You may also discover another issue with this data set, that I won't say anything about (yet, and except for a little hint that's relevant here) -- we want to see if anyone notices it.  The hint is that it's possible to get over 99% accuracy using only the *real* predictors -- I did.  That is due to a quirk of the data, which you are invited to figure out.  Also, don't expect that sort of accuracy on other data sets."


nodesize = 14 to begin with for performance, but changed to 1 for classification accuracy. 
changed trainControl, number = 1 or 3 to prevent cross-validation (or remove bootstrapping?) for initial selection of model


Comparison of classification models with default settings shows that random forest is a good approach that gains 99% accuracy with default settings through the caret package. Principal Component Analysis (PCA) could be applicable to this kind of sensor data. Using a default threshold of 95% does reduce the number of factors from 52 to 25, achieving similar levels of accuracy, but the difference is not significant so we will proceed without PCA.

Iterating through values for mtry, and optimizing for test set accuracy shows gives an optimal value of 9.



3. Try decision trees - it might be easy, to find the quirk in the data



```{r rf, cache = TRUE, eval = FALSE}
set.seed(1234)
ctrl <- trainControl(number = 3)
ifelse(file.exists("rf.rds"),
       rf.fit <- readRDS("rf.rds"),
       rf.fit <- train(bc.train, 
                       classe.train,
                       method = "rf",
                       ncores = 2,
                       importance = TRUE))
rf.predict <- predict(rf.fit, bc.test1)
confusionMatrix(rf.predict, classe.test1)
rf.fit
rf.fit$time$everything[3]/60
if(!file.exists("rf.rds")) saveRDS(rf.fit, file="rf.rds")
```


```{r mtry, cache = TRUE, eval = FALSE}
set.seed(848)
n.comparison <- data.frame()
for(n in 1:15) { 
  s <- system.time(rfm.fit <- randomForest(bc.train, classe.train, mtry = 9, nodesize = n))
  rfm.predict <- predict(rfm.fit, bc.test1)
  n.comparison <- rbind(n.comparison,
                        c(n, s, confusionMatrix(rfm.predict, classe.test1)$overall))
}
names(n.comparison) <- c("nodesize", "Duration", "Accuracy", "Kappa", "AccuracyLower", "AccuracyUpper", "AccuracyNull", "AccuracyPValue", "McnemarPValue")
n.comparison
```

Report covers:
1. Model design and reasons
3. Expected out of sample error - estimate the error with cross-validation
5. 20 results
6. Compiled html

Overfitting is in our favour - since the validation data is sampled from the training set, the training data is very indicative of the test data.


1. trainControl


# Model analysis







```{r Importance, cache = TRUE, eval = FALSE}
varImpPlot(rf.fit$finalModel)
```

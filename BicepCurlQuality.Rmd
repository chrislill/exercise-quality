---
title: "Bicep curl quality"
author: "Chris Lill"
date: "17 September 2015"
output: html_document
---

# Executive summary
A random forest model has been created to predict the quality of a bicep curl based on sensor data. This model was tuned and is expected to have an out of sample error of ?????

The original study that generated this data is [Wearable Computing: Accelerometers' Data Classification of Body Postures and Movements](http://groupware.les.inf.puc-rio.br/work.jsf?p1=10335) by Ugulino, Cardador, Vega, Velloso, Milidiu, and Fuks. The study categorised bicep curls in the following categories:

* A - exactly according to the specification 
* B - throwing the elbows to the front
* C - lifting the dumbbell only halfway
* D - lowering the dumbbell only halfway
* E - throwing the hips to the front

# Data preparation
The project supplied a large training set, and a small set of 20 records to be predict. For initial investigations and model selection the training set was split with 70% of data to train the model, and 30% to test it. For the final model the full training set was used.

Initial exploration of the data shows that the study derived many factors where 98% of the data is NA or blank. These factors can be removed from the model, along with all metadata, leaving 52 factors for modelling.

```{r Download, cache = TRUE, echo = FALSE}
train.file <- "pml-training.csv"
validation.file <- "pml-testing.csv"
if (!file.exists(train.file)) {
folder.url <- "https://d396qusza40orc.cloudfront.net/predmachlearn/"
    download.file(paste(folder.url,train.file, sep = ""), train.file)
    download.file(paste(folder.url,validation.file, sep = ""), validation.file)
    date.downloaded = Sys.Date()
}
full.train <- read.csv(train.file)
bc.validation <- read.csv(validation.file)
```

```{r RemoveNullFactors, cache = TRUE, echo = FALSE}
all.factors <- names(full.train)
remove.names <- c("X",
                  "user_name",
                  "num_window",
                  "raw_timestamp_part_1",
                  "raw_timestamp_part_2",
                  "cvtd_timestamp",
                  "new_window",
                  "*_roll_belt",
                  "*_yaw_belt",
                  "*_pitch_belt",
                  "*_picth_belt",
                  "*_roll_arm",
                  "*_pitch_arm",
                  "*_picth_arm",
                  "*_yaw_arm",
                  "*_roll_dumbbell",
                  "*_pitch_dumbbell",
                  "*_picth_dumbbell",
                  "*_yaw_dumbbell",
                  "*_roll_forearm",
                  "*_pitch_forearm",
                  "*_picth_forearm",
                  "*_yaw_forearm",
                  "var_accel_",
                  "var_total_accel_belt")
remove.regex <- paste(remove.names, collapse = "|")
remove.factors <- grepl(remove.regex, all.factors)
bc.train <- full.train[,!remove.factors]

# Separate the outcome from the predictors, because this is a more performant way to call random forest
classe.train <- bc.train[, 53]
bc.train <- bc.train[, -53]
```

# Exploratory data analysis

The following plots show the variability by classification for six factors with the greatest importance in the final model. They show that there are visible differences, but that a combination of factors will be needed for an accurate prediction.

```{r Exploratory, cache = TRUE, echo = FALSE, fig.width=10}
library(ggplot2, quietly = TRUE)
library(gridExtra)

b1 <- geom_boxplot(aes(colour = classe))
b2 <- theme(legend.position="none", axis.title.x = element_blank())
b3 <- scale_colour_brewer(palette = "Set1")

g1 <- ggplot(full.train, aes(classe, roll_belt)) + b1 + b2 + b3
g2 <- ggplot(full.train, aes(classe, yaw_belt)) + b1 + b2 + b3
g3 <- ggplot(full.train, aes(classe, pitch_belt)) + b1 + b2 + b3
g4 <- ggplot(full.train, aes(classe, magnet_dumbbell_z)) + b1 + b2 + b3
g5 <- ggplot(full.train, aes(classe, magnet_dumbbell_x)) + b1 + b2 + b3
g6 <- ggplot(full.train, aes(classe, pitch_forearm)) + b1 + b2 + b3
grid.arrange(g1, g2, g3, g4, g5, g6, ncol = 3, bottom = "Figure 1: Boxplots of six important factors by classification")
```

# Model selection
The choice of machine learning algorithm was carried out with caret using different methods including random forest, random forest with principal component analysis (pca) and neural networks. Random forest was the most accurate with a suspiciously high accuracy of 99%.

The algorithm was tuned for nodesize. Performance of the algorithm was slow using default settings for caret, since a nodesize of 1 generates large trees and cross-validation is repeated on 25 samples. In this case randomForest() was used. In future analysis I would use the caret package with `train(method = "rf", nodesize = 10)` and `trainControl(method = "null")` Investigation into nodesize illustrated the trade off between performance and accuracy as it approaches 1. Nodesize = 1 will be used in the final model.

The algorith was tuned for mtry. Values of mtry from 1 to 15 were used, and mtry = 9 was found to be the optimum. This code is shown below for information, but not evaluated. It requires that data is split into a training and test set. In future analysis I would use the caret package with `train(method = "rf", tuneGrid = data.frame(mtry = 1:15))`.

```{r mtry, cache = TRUE, eval = FALSE}
set.seed(848)
m.comparison <- data.frame()
for(m in 1:15) { 
  s <- system.time(rfm.fit <- randomForest(bc.train, classe.train, mtry = m, nodesize = 10))
  rfm.predict <- predict(rfm.fit, bc.test)
  n.comparison <- rbind(m.comparison,
                        c(m, confusionMatrix(rfm.predict, classe.test1)$overall))
}
names(m.comparison) <- c("mtry", "Accuracy", "Kappa", "AccuracyLower", "AccuracyUpper", "AccuracyNull", "AccuracyPValue", "McnemarPValue")
m.comparison
```

Given the relationship between the sensor data in different dimensions, principal component analysis (pca) would appear to be a valuable approach. Investigation showed that a default threshold of 95% reduces the number of factors from 52 to 25, achieving similar levels of accuracy. The improvement in accuracy is not significant so pca will not be used.

The selected model is a randomForest with mtry = 9 and nodesize = 1. It is built using Caret, with cross validation resampling using 10 folds. 

```{r FinalModel, eval = FALSE}
library(caret, quietly = TRUE)
set.seed(29)
ctrl <- trainControl(method = "cv")
ifelse(file.exists("final.rds"),
       bc.fit <- readRDS("final.rds"),
       bc.fit <- train(bc.train, 
                       classe.train, 
                       method = "rf", 
                       tuneGrid = data.frame(mtry = 9)
                       tuneLength = 1))
if(!file.exists("final.rds")) saveRDS(bc.fit, file="final.rds")
```

```{r a}
bc.fit <- readRDS("rf.rds")
```

# Model analysis
10-fold cross validation was carried out across the whole training dataset on a model with a single set of parameters. The estimate for the out of sample accuracy is `r signif(bc.fit$results[1, "Accuracy"],3)`. The 95% confidence interval for the accuracy is `r signif((bc.fit$results[1, "Accuracy"] + c(-1, 1) * qnorm(0.975) * bc.fit$results[1, "AccuracySD"]),3)`.

This model is a suspiciously good fit, which is a good indicator of overfitting. In this case the 19622 observations are only split across 6 participants, 5 variants, and 10 repetitions. This means that there are ~60 observations for every bicep curl. When predicting one of these observations it can be easily matched to the other set of observations using noise in each repetition. It would be less accurate at predicting observations for new participants, or even additional bicep curls.

One way of improving the model so that it is more generalizable would be to select specific factors which are indicative of each variation. Understanding of human musculoskeletal system would be very beneficial here.

The following figures show the confusion matrix and variable importance for the final model.

```{r Confusion, echo = FALSE}
require(knitr, quietly = TRUE)
kable(bc.fit$finalModel$confusion, 
      digits = 3,
      caption = "Figure 2: Confusion matrix for the final model",
      col.names = c("A", "B", "C", "D", "E", "Classification error"))
```




```{r Importance, cache = TRUE, echo = FALSE, fig.height =8, fig.width = 10}
varImpPlot(bc.fit$finalModel, main = "Figure 3: Importance of each factor in the final model")
```


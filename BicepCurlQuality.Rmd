---
title: "Bicep curl quality"
author: "Chris Lill"
date: "17 September 2015"
output: html_document
---

#


http://groupware.les.inf.puc-rio.br/har
http://groupware.les.inf.puc-rio.br/work.jsf?p1=11201

A: exactly according to the specification 
B: throwing the elbows to the front
C: lifting the dumbbell only halfway
D: lowering the dumbbell only halfway
E: throwing the hips to the front

```{r Download, cache = TRUE}
train.file <- "pml-training.csv"
test.file <- "pml-testing.csv"
if (!file.exists(train.file)) {
folder.url <- "https://d396qusza40orc.cloudfront.net/predmachlearn/"
    download.file(paste(folder.url,train.file, sep = ""), train.file)
    download.file(paste(folder.url,test.file, sep = ""), test.file)
    date.downloaded = Sys.Date()
}

raw.train <- read.csv(train.file)
raw.test2 <- read.csv(test.file)
```

The study derived many factors from the original data where 98% of the data is NA or blank. We'll remove these factors from the model, along with all metadata.

```{r RemoveNullFactors, cache = TRUE}
all.factors <- names(raw.train)
remove.names <- c("X",
                  "user_name",
                  "num_window",
                  "raw_timestamp_part_1",
                  "raw_timestamp_part_2",
                  "cvtd_timestamp",
                  "new_window",
                  "*_roll_belt",
                  "*_yaw_belt",
                  "*_pitch_belt",
                  "*_picth_belt",
                  "*_roll_arm",
                  "*_pitch_arm",
                  "*_picth_arm",
                  "*_yaw_arm",
                  "*_roll_dumbbell",
                  "*_pitch_dumbbell",
                  "*_picth_dumbbell",
                  "*_yaw_dumbbell",
                  "*_roll_forearm",
                  "*_pitch_forearm",
                  "*_picth_forearm",
                  "*_yaw_forearm",
                  "var_accel_",
                  "var_total_accel_belt")
remove.regex <- paste(remove.names, collapse = "|")
remove.factors <- grepl(remove.regex, all.factors)

full.train <- raw.train[,!remove.factors]
```

We will use 30% of the training data to validate and tune our model, and call this `ex.test1`. The validation set provided by Coursera will be called `ex.test2`

```{r SplitData, cache = TRUE}
library(caret)
set.seed(29)
inTrain <- createDataPartition(full.train$classe, p = 0.7, list = FALSE)
ex.train <- full.train[inTrain, -53]
ex.test1 <- full.train[-inTrain, -53]
classe.train <- full.train[inTrain, 53]
classe.test1 <- full.train[-inTrain, 53]
```



8. Quirk of data that enables 99% accuracy
"You may also discover another issue with this data set, that I won't say anything about (yet, and except for a little hint that's relevant here) -- we want to see if anyone notices it.  The hint is that it's possible to get over 99% accuracy using only the *real* predictors -- I did.  That is due to a quirk of the data, which you are invited to figure out.  Also, don't expect that sort of accuracy on other data sets."


nodesize = 14 to begin with for performance, but changed to 1 for classification accuracy. 
changed trainControl, number = 1 or 3 to prevent cross-validation (or remove bootstrapping?) for initial selection of model


Comparison of classification models with default settings shows that random forest is a good approach that gains 99% accuracy with default settings through the caret package. Principal Component Analysis (PCA) could be applicable to this kind of sensor data. Using a default threshold of 95% does reduce the number of factors from 52 to 25, achieving similar levels of accuracy, but the difference is not significant so we will proceed without PCA.

Iterating through values for mtry, and optimizing for test set accuracy shows gives an optimal value of 9.



3. Try decision trees - it might be easy, to find the quirk in the data



```{r rf, cache = TRUE}
set.seed(1234)
ctrl <- trainControl(number = 3)
ifelse(file.exists("rf.rds"),
       rf.fit <- readRDS("rf.rds"),
       rf.fit <- train(ex.train, 
                       classe.train,
                       method = "rf",
                       ncores = 2,
                       importance = TRUE))
rf.predict <- predict(rf.fit, ex.test1)
confusionMatrix(rf.predict, classe.test1)
rf.fit
rf.fit$time$everything[3]/60
if(!file.exists("rf.rds")) saveRDS(rf.fit, file="rf.rds")
```


```{r mtry, cache = TRUE}
set.seed(848)
n.comparison <- data.frame()
for(n in 1:15) { 
  s <- system.time(rfm.fit <- randomForest(ex.train, classe.train, mtry = 9, nodesize = n))
  rfm.predict <- predict(rfm.fit, ex.test1)
  n.comparison <- rbind(n.comparison,
                        c(n, s, confusionMatrix(rfm.predict, classe.test1)$overall))
}
names(n.comparison) <- c("nodesize", "Duration", "Accuracy", "Kappa", "AccuracyLower", "AccuracyUpper", "AccuracyNull", "AccuracyPValue", "McnemarPValue")
n.comparison
```

Report covers:
1. Model design and reasons
3. Expected out of sample error - estimate the error with cross-validation
5. 20 results
6. Compiled html

Overfitting is in our favour - since the validation data is sampled from the training set, the training data is very indicative of the test data.


1. trainControl








```{r Exploratory, cache = TRUE}
library(ggplot2)
library(gridExtra)

b1 <- geom_boxplot(aes(colour = classe))
b2 <- theme(legend.position="none", axis.title.x = element_blank())
b3 <- scale_colour_brewer(palette = "Set1")

g1 <- ggplot(raw.train, aes(classe, roll_belt)) + b1 + b2 + b3
g2 <- ggplot(raw.train, aes(classe, yaw_belt)) + b1 + b2 + b3
g3 <- ggplot(raw.train, aes(classe, pitch_belt)) + b1 + b2 + b3
g4 <- ggplot(raw.train, aes(classe, magnet_dumbbell_z)) + b1 + b2 + b3
g5 <- ggplot(raw.train, aes(classe, magnet_dumbbell_x)) + b1 + b2 + b3
g6 <- ggplot(raw.train, aes(classe, pitch_forearm)) + b1 + b2 + b3
grid.arrange(g1, g2, g3, g4, g5, g6, ncol = 3)
```

```{r Importance, cache = TRUE}
varImpPlot(rf.fit$finalModel)
```

---
title: "Bicep curl quality"
author: "Chris Lill"
date: "17 September 2015"
output: html_document
---

# Executive summary
A random forest model has been created to predict the quality of a bicep curl based on sensor data. This model was tuned and is expected to have an out of sample error of ?????

The original study that generated this data is [Wearable Computing: Accelerometers' Data Classification of Body Postures and Movements](http://groupware.les.inf.puc-rio.br/work.jsf?p1=10335) by Ugulino, Cardador, Vega, Velloso, Milidiu, and Fuks. The study categorised bicep curls in the following categories:

* A - exactly according to the specification 
* B - throwing the elbows to the front
* C - lifting the dumbbell only halfway
* D - lowering the dumbbell only halfway
* E - throwing the hips to the front

# Data preparation
The project supplied a large training set, and a small set of 20 records to be predict, which will be used as follows:

* `bc.train` contains 70% of the large dataset to train the model,
* `bc.test` contains 30% of the large dataset to tune the model,
* `bc.validation` contains the 20 records to be predicted.

Initial investigation shows that the study derived many factors from the original data where 98% of the data is NA or blank. These factors can be removed from the model, along with all metadata, leaving 52 factors for modelling.

```{r Download, cache = TRUE, echo = FALSE}
train.file <- "pml-training.csv"
validation.file <- "pml-testing.csv"
if (!file.exists(train.file)) {
folder.url <- "https://d396qusza40orc.cloudfront.net/predmachlearn/"
    download.file(paste(folder.url,train.file, sep = ""), train.file)
    download.file(paste(folder.url,validation.file, sep = ""), validation.file)
    date.downloaded = Sys.Date()
}

full.train <- read.csv(train.file)
bc.validation <- read.csv(validation.file)
```

```{r RemoveNullFactors, cache = TRUE, echo = FALSE}
all.factors <- names(full.train)
remove.names <- c("X",
                  "user_name",
                  "num_window",
                  "raw_timestamp_part_1",
                  "raw_timestamp_part_2",
                  "cvtd_timestamp",
                  "new_window",
                  "*_roll_belt",
                  "*_yaw_belt",
                  "*_pitch_belt",
                  "*_picth_belt",
                  "*_roll_arm",
                  "*_pitch_arm",
                  "*_picth_arm",
                  "*_yaw_arm",
                  "*_roll_dumbbell",
                  "*_pitch_dumbbell",
                  "*_picth_dumbbell",
                  "*_yaw_dumbbell",
                  "*_roll_forearm",
                  "*_pitch_forearm",
                  "*_picth_forearm",
                  "*_yaw_forearm",
                  "var_accel_",
                  "var_total_accel_belt")
remove.regex <- paste(remove.names, collapse = "|")
remove.factors <- grepl(remove.regex, all.factors)

full.train <- full.train[,!remove.factors]
```

```{r SplitData, cache = TRUE, echo = FALSE}
library(caret, quietly = TRUE)
set.seed(29)
inTrain <- createDataPartition(full.train$classe, p = 0.7, list = FALSE)
bc.train <- full.train[inTrain, -53]
bc.test <- full.train[-inTrain, -53]
classe.train <- full.train[inTrain, 53]
classe.test <- full.train[-inTrain, 53]
```

# Exploratory data analysis

The following plots show the variability by classification for six factors with the greatest importance in the final model. They show that there are visible differences, but that a combination of factors will be needed for an accurate prediction.

```{r Exploratory, cache = TRUE, echo = FALSE, fig.width=10}
library(ggplot2, quietly = TRUE)
library(gridExtra)

b1 <- geom_boxplot(aes(colour = classe))
b2 <- theme(legend.position="none", axis.title.x = element_blank())
b3 <- scale_colour_brewer(palette = "Set1")

g1 <- ggplot(full.train, aes(classe, roll_belt)) + b1 + b2 + b3
g2 <- ggplot(full.train, aes(classe, yaw_belt)) + b1 + b2 + b3
g3 <- ggplot(full.train, aes(classe, pitch_belt)) + b1 + b2 + b3
g4 <- ggplot(full.train, aes(classe, magnet_dumbbell_z)) + b1 + b2 + b3
g5 <- ggplot(full.train, aes(classe, magnet_dumbbell_x)) + b1 + b2 + b3
g6 <- ggplot(full.train, aes(classe, pitch_forearm)) + b1 + b2 + b3
grid.arrange(g1, g2, g3, g4, g5, g6, ncol = 3, bottom = "Figure 1: Boxplots of six important factors by classification")
```

# Model selection
The choice of Machine Learning algorithm was carried out with caret using different methods including random forest, random forest with principal component analysis (pca) and neural networks. Random forest was the most accurate with a suspiciously high accuracy of 99%.

The algorithm was tuned for nodesize. Performance of the algorithm was slow using default settings for caret, since a nodesize of 1 generates big trees and cross-validation is repeated on 25 samples. In this case randomForest() was used. In future analysis I would use the caret package with `train(method = "rf", nodesize = 10)` and `trainControl(method = "null")` Investigation into nodesize illustrated the trade off between performance and accuracy as it approaches 1. Nodesize = 1 will be used in the final model.

The algorith was tuned for mtry. Values of mtry from 1 to 15 were used, and mtry = 9 was found to be the optimum. This code is shown below for information, but not evaluated. In future analysis I would use the caret package with `train(method = "rf", tuneGrid = data.frame(mtry = 2:15))`.

```{r mtry, cache = TRUE, eval = FALSE}
set.seed(848)
m.comparison <- data.frame()
for(m in 1:15) { 
  s <- system.time(rfm.fit <- randomForest(bc.train, classe.train, mtry = m, nodesize = 10))
  rfm.predict <- predict(rfm.fit, bc.test)
  n.comparison <- rbind(m.comparison,
                        c(m, confusionMatrix(rfm.predict, classe.test1)$overall))
}
names(m.comparison) <- c("mtry", "Accuracy", "Kappa", "AccuracyLower", "AccuracyUpper", "AccuracyNull", "AccuracyPValue", "McnemarPValue")
m.comparison
```

Given the relationship between the sensor data in different dimensions, principal component analysis (pca) would appear to be a valuable approach. Investigation showed that a default threshold of 95% reduces the number of factors from 52 to 25, achieving similar levels of accuracy. The improvement in accuracy is not significant so pca will not be used.

Combine test and train

Create final model using caret





# Model analysis
3. Expected out of sample error - estimate the error with cross-validation

Overfitting is in our favour - since the validation data is sampled from the training set, the training data is very indicative of the test data.

```{r Importance, cache = TRUE, eval = FALSE}
varImpPlot(rf.fit$finalModel)
```


# Prediction assignment
5. 20 results


Finally, compile html















